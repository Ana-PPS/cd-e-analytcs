{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ana-PPS/data-and-analytcs/blob/MVP_II/MVPb_SprintII_AnaPaulaSalgado_entrega.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PUC Rio: Pós Graduação em Ciência de Dados e Analytics**\n",
        "\n",
        "**MVP Sprint II: Machine Learning & Analytics**\n",
        "**Parte b**\n",
        "\n",
        "Aluna: Ana Paula Pinheiro Salgado\n",
        "\n",
        "Julho/2023"
      ],
      "metadata": {
        "id": "XRILPeMPQymh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVbw0VVzq9al"
      },
      "source": [
        "## Seção I: Introdução\n",
        "**Classificador de imagens multiclasse - Animais**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJrdI-umq9an"
      },
      "source": [
        "**Contexto:** Temos um conjunto de imagens que representam 4 categorias de animais: vaca, galinha, cavalo e elefante. O objetivo deste notebook é criar um modelo de visão computacional que consiga classificar uma imagem dentre essas categorias.\n",
        "\n",
        "**Estrutura:** O notebook encontra-se dividido da seguinte forma:\n",
        "\n",
        "- Importação das bibliotecas\n",
        "- Acesso e tratamento dos dados que serão a entrada do modelo de deep learning\n",
        "- Configuração do modelo de deep learning usando uma rede neural convolucional simples com Keras\n",
        "- Treinamento do modelo de deep learning\n",
        "- Execução do modelo de deep learning treinado\n",
        "- Avaliação do modelo de deep learning\n",
        "- Exportação do modelo de deep learning\n",
        "- Teste do modelo exportado\n",
        "\n",
        "**Dataset:** A partir do dataset original, baixado do Kaggle (https://www.kaggle.com/datasets/alessiocorrado99/animals10), foram selecionadas apenas 4 pastas de imagens de forma a otimizar o tempo de execução do MVP."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyu90mQIq9ap"
      },
      "source": [
        "## Importando as bibliotecas necessárias para executar o notebook\n",
        "(Serão utilizadas as bibliotecas pandas e numpy, para a manipulação dos dados; matplotlib, para geração de gráficos; os, para manipulação de pastas e diretórios e bibliotecas voltadas para Machine e Deep Learning, tais como Keras, Tensor Flow e Scikit-Learn)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xuBOf55bq9ap"
      },
      "outputs": [],
      "source": [
        "# para usar o Google Drive\n",
        "!pip install -q gdown\n",
        "import gdown\n",
        "from google.colab import drive\n",
        "\n",
        "#para acessar e manipular arquivos, diretórios e estrutra de dados\n",
        "import os\n",
        "import shutil\n",
        "from zipfile import ZipFile\n",
        "import pandas as pd\n",
        "\n",
        "# cálculos numéricos e operações matemáticas e trabalhar com números aleatórios\n",
        "import numpy as np\n",
        "import math\n",
        "import random\n",
        "\n",
        "# bibliotecas do keras para pré-processamento, modelos convolucionais e otimizadores dos modelos\n",
        "!pip install -q tensorflow\n",
        "!pip install -q keras\n",
        "from tensorflow import keras\n",
        "from keras.optimizers import Adam\n",
        "from keras.preprocessing import image\n",
        "from keras.layers.experimental import preprocessing\n",
        "from keras.layers import BatchNormalization\n",
        "from keras import layers,models,Model\n",
        "from keras.applications import VGG16\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras import layers\n",
        "from keras import models\n",
        "from keras.preprocessing import image as keras_image\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "import tensorflow as tf\n",
        "\n",
        "# sckit-learn para pré-processamento e uso de métricas em machine learning\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.utils import class_weight, shuffle\n",
        "from sklearn.metrics import confusion_matrix, classification_report, balanced_accuracy_score\n",
        "import sklearn.metrics as skm\n",
        "\n",
        "# plotagem de gráficos, visualizações e imagens\n",
        "from PIL import Image\n",
        "import matplotlib.image as mpimg\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from IPython.display import display\n",
        "from skimage.io import imread\n",
        "from skimage.transform import resize\n",
        "\n",
        "# iterações\n",
        "import itertools\n",
        "\n",
        "# para uso de data e hora\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Acessando e tratando os dados que serão a entrada do modelo de deep learning"
      ],
      "metadata": {
        "id": "KEkc_j58RU6i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Baixando a pasta compactada para o notebook\n",
        "file_id = \"1bg_JhbuWv2lCCDyrfGI2yr5dROxbijYQ\"\n",
        "\n",
        "folder_path = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "output = \"4animais.zip\"\n",
        "gdown.download(folder_path, output)"
      ],
      "metadata": {
        "id": "UcY4ZgosYDsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Descompactando o arquivo\n",
        "with ZipFile('4animais.zip', 'r') as zip_object:\n",
        "  zip_object.extractall()"
      ],
      "metadata": {
        "id": "7X1Ott8sn5vS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reunindo todos os arquivos em uma pasta para transformar em um dataset\n",
        "\n",
        "path = \"/content/\"\n",
        "\n",
        "classes = ['vaca', 'galinha', 'elefante', 'cavalo']\n",
        "\n",
        "foldernames = os.listdir(path)\n",
        "\n",
        "data = {\"images\": [], \"animal\": []}\n",
        "\n",
        "for folder in classes:\n",
        "    folderpath = os.path.join(path, folder)\n",
        "    filelist = os.listdir(folderpath)\n",
        "    for file in filelist:\n",
        "        fpath = os.path.join(folderpath, file)\n",
        "        data[\"images\"].append(fpath)\n",
        "        data[\"animal\"].append(folder)\n",
        "\n",
        "\n",
        "df = pd.DataFrame(data)"
      ],
      "metadata": {
        "id": "sivZTePOYHGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.head)\n",
        "print(df.shape)"
      ],
      "metadata": {
        "id": "7ZLR0vKW9Nw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Resumo do dataset criado\n",
        "\n",
        "print(\"Tamanho do dataset: \", df.shape)\n",
        "print(\"_______________________________________\")\n",
        "print(\"Valores null: \")\n",
        "print(df.isnull().sum())\n",
        "print(\"_______________________________________\")\n",
        "print(\"Valores únicos: \")\n",
        "print(df.nunique())\n",
        "\n",
        "print(\"_______________________________________\")\n",
        "print(\"Qnt de imagens por categoria : \")\n",
        "print(df.animal.value_counts())\n",
        "\n",
        "print(\"_______________________________________\")\n",
        "print(\"Informação do dataset: \")\n",
        "print(df.info())"
      ],
      "metadata": {
        "id": "gP-JbVIqYHBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualizando algumas imagens do conjunto de dados\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(3*4, 3*4)\n",
        "for i, row in df.sample(n=10).reset_index().iterrows():\n",
        "    plt.subplot(2,5,i+1)\n",
        "    image_path = row['images']\n",
        "    image = Image.open(image_path)\n",
        "    plt.imshow(image)\n",
        "    plt.title(row[\"animal\"])\n",
        "    plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "G07ArvIwYG5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separação do dataframe em treino, teste e validação\n",
        "df_train, Temp_df = train_test_split(df, train_size=0.7, random_state=13, shuffle=True)\n",
        "df_test, df_val = train_test_split(Temp_df, test_size=0.6, random_state=30, shuffle=True)"
      ],
      "metadata": {
        "id": "yNm6NziRZYDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Resumo da separação do conjunto de dados\n",
        "print(\"#########Train##############\")\n",
        "print(df_train.head())\n",
        "print(df_train.shape)\n",
        "print(\"#########Test###############\")\n",
        "print(df_test.head())\n",
        "print(df_test.shape)\n",
        "print(\"#########Validação###############\")\n",
        "print(df_val.head())\n",
        "print(df_val.shape)"
      ],
      "metadata": {
        "id": "UMrAekGIZq9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rs2lLQhJq9au"
      },
      "source": [
        "##  Preparação dos dados\n",
        "\n",
        "`ImageDataGenerator` é uma classe utilitária fornecida pelo TensorFlow para aumentar e pré-processar dados de imagens. É comumente usada em tarefas de deep learning, especialmente para classificação de imagens, para gerar imagens aumentadas (data augmentation) sob demanda durante o treinamento do modelo.\n",
        "\n",
        "O Data Augmentation aplica transformações aleatórias nas imagens existentes, o que ajuda a evitar overfitting e torna o modelo mais robusto, expondo-o a uma variedade maior de variações nas imagens.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n5IcBa8Qq9at"
      },
      "outputs": [],
      "source": [
        "# Definindo o tamanho do batch, a dimensão das imagens e a quantidade de épocas\n",
        "batch_size = 32\n",
        "img_height = 224\n",
        "img_width = 224\n",
        "num_classes = 4\n",
        "epochs = 30"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_datagen=ImageDataGenerator(rescale=1./255,\n",
        "                             rotation_range=40,\n",
        "                             shear_range=0.2,\n",
        "                             zoom_range=0.2,\n",
        "                             horizontal_flip=True,\n",
        "                             fill_mode='nearest')\n",
        "\n",
        "val_datagen=ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "test_datagen=ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "\n",
        "train_generator=train_datagen.flow_from_dataframe(\n",
        "    dataframe=df_train,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    x_col='images',\n",
        "    y_col='animal',\n",
        "    color_mode ='rgb',\n",
        "    seed = 13,\n",
        "    shuffle=False\n",
        "    )\n",
        "\n",
        "val_generator=val_datagen.flow_from_dataframe(\n",
        "    dataframe=df_val,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    x_col='images',\n",
        "    y_col='animal',\n",
        "    color_mode ='rgb',\n",
        "    shuffle=False\n",
        "    )\n",
        "\n",
        "test_generator=test_datagen.flow_from_dataframe(\n",
        "    dataframe = df_test,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    x_col='images',\n",
        "    y_col='animal',\n",
        "    color_mode ='rgb',\n",
        "    shuffle=False)"
      ],
      "metadata": {
        "id": "rlIamgUyaLAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Seção III: Configuração do Modelo de Deep Learning"
      ],
      "metadata": {
        "id": "Zs7GKCouSY2E"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiKBVra3q9aw"
      },
      "source": [
        "### Configuração de um modelo de deep learning usando uma rede neural convolucional (CNN) simples com a biblioteca Keras\n",
        "\n",
        "Aqui foi especificada uma `camada convolucional 2D` que possui 32 filtros(depois 64, 128 e 256) e função de ativação `ReLU`. Na sequência é adicionada uma camada `softmax` com a mesma função de ativação.  O parâmetro `input_shape` define a forma das imagens de entrada para a rede."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x0DUnDN6q9aw"
      },
      "outputs": [],
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(img_height,img_width,3)),\n",
        "    keras.layers.MaxPooling2D((2,2)),\n",
        "    keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    keras.layers.MaxPooling2D((2,2)),\n",
        "    keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    keras.layers.MaxPooling2D((2,2)),\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(128, activation='relu'),\n",
        "    keras.layers.Dense(num_classes, activation='softmax')\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Resumindo o modelo que será utilizado\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "JcBoJmn6Su1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2rYRq18q9ax"
      },
      "source": [
        "### Treinamento do modelo de deep learning\n",
        "\n",
        "#### Compilação do modelo Keras com as configurações do treinamento.\n",
        "\n",
        "`optimizer='adam'`: especifica o otimizador a ser usado durante o treinamento.\n",
        "\n",
        "`loss='categorical_crossentropy'`: especifica a função de perda a ser usada durante o treinamento. Para problemas de classificação multiclasse, onde a variável alvo tem mais de duas categorias, a perda categórica de entropia cruzada é freqüentemente empregada.\n",
        "\n",
        "`metrics=['accuracy']`: especifica as métricas de avaliação a serem usadas durante o treinamento e o teste."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nEVYlto7q9ax"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WIrrinDwq9ay"
      },
      "outputs": [],
      "source": [
        "history = model.fit(train_generator, epochs=epochs, validation_data=val_generator)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Visualização de métricas da avaliação do modelo treinado"
      ],
      "metadata": {
        "id": "DIDZSgRwFF7e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['loss'], label=\"Training Loss\")\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FsHUXgn0E_Wa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['accuracy'], label=\"Training Accuracy\")\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "y1mrMa6_LnWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O modelo na primeira arquitetura parece começar a apresentar overfitting, dado que a acurácia tende a um resultado bem superior no conjunto de treinamento, com tendência inversa no conjunto de validação. Começando a apresentar divergência de resultados a partir da 10ª época.\n",
        "\n",
        "Para tentar melhorar a generalização e reduzir overfitting, será aplicado algumas camadas de dropout e uma camada Flatten para transformar os dados em um único vetor, preparando-os para camada Densa posterior."
      ],
      "metadata": {
        "id": "MT66FuKaL3C0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Adaptação do 1º modelo"
      ],
      "metadata": {
        "id": "DttXKvUINZS-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.Dropout(0.25, input_shape=(img_height,img_width, 3), seed = 13),\n",
        "    keras.layers.Conv2D(32, (3,3), padding='same', activation='relu'),\n",
        "    keras.layers.MaxPooling2D((2,2)),\n",
        "    keras.layers.Conv2D(64, (3,3), padding='same', activation='relu'),\n",
        "    keras.layers.MaxPooling2D((2,2)),\n",
        "    keras.layers.Conv2D(128, (3,3), padding='same', activation='relu'),\n",
        "    keras.layers.MaxPooling2D((2,2)),\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(128, activation='relu'),\n",
        "    keras.layers.Dropout(0.25)\n",
        "    keras.layers.Dense(32, activation='relu')\n",
        "    keras.layers.Dense(num_classes, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "XZ6gQ8kOL2R9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Salvando o Modelo para uso posterior\n",
        "\n",
        "para uso posterior, não exigido no check list"
      ],
      "metadata": {
        "id": "zCS9ilGpJ1_q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# obtendo data e hora atual\n",
        "#now = datetime.now()\n",
        "\n",
        "# Definição do formato\n",
        "#formato_hora = %Y-%m-%dT%H%M\n",
        "\n",
        "# Converte a data e hora em uma string com formato específico\n",
        "#datetime_formatado = now.strftime(formato_hora)\n",
        "\n",
        "# Mount Google Drive\n",
        "#drive.mount('content', force_remount=True)\n",
        "\n",
        "#Defineo caminho do diretório para salvar o modelo\n",
        "#save_path = 'content'\n",
        "\n",
        "#nome_modelo = 'trained_model_' + datetime_formatado + '.h5'\n",
        "\n",
        "# Salva o modelo\n",
        "#model.save(save_path + name_model)\n"
      ],
      "metadata": {
        "id": "g8S_N6raGdiL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhbNC_28q9ay"
      },
      "source": [
        "## 5. Execução do modelo de deep learning treinado no teste_generator"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count_images = 0\n",
        "class_names = ['vaca', 'galinha', 'elefante', 'cavalo']\n",
        "y_pred = list()\n",
        "y_true = list()\n",
        "\n",
        "\n",
        "# Percorrendo a pasta onde estão salvas as imagens de teste\n",
        "\n",
        "for img_path, label in zip(df_test['images'], df_test['animal']):\n",
        "\n",
        "    if img_path.endswith('.jpeg') or img_path.endswith('.jpg') or img_path.endswith('.png'):\n",
        "\n",
        "            split_path = img_path.split('/')\n",
        "            label = split_path[2]\n",
        "            y_true.append(label)\n",
        "\n",
        "            display(Image.open(img_path).resize((300, 300)))\n",
        "            img = image.load_img(img_path, target_size=(img_height, img_width))\n",
        "            x = image.img_to_array(img)\n",
        "            x = np.expand_dims(x, axis=0)\n",
        "            x = x.astype('float32') / 255.0\n",
        "\n",
        "            # Previsão\n",
        "            prediction = model.predict(x)\n",
        "\n",
        "            # Printando as saídas do modelo\n",
        "            predicted_class = np.argmax(prediction[0])\n",
        "            probability = prediction[0][predicted_class]\n",
        "            y_pred.append(class_names[predicted_class])\n",
        "            print(\"Label:\", label)\n",
        "            print(\"Previsão:\", class_names[predicted_class])\n",
        "            print(\"Probabilidade:\", probability)\n",
        "            print(\"\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "mWrPmjh0V-Y4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculando as métricas\n",
        "accuracy = skm.accuracy_score(y_true, y_pred)\n",
        "precision = skm.precision_score(y_true, y_pred, average='weighted')\n",
        "recall = skm.recall_score(y_true, y_pred, average='weighted')\n",
        "f1score = skm.f1_score(y_true, y_pred, average='weighted')\n",
        "\n",
        "print(\"Accuracy: \", accuracy)\n",
        "print(\"Precision: \", precision)\n",
        "print(\"Recall: \", recall)\n",
        "print(\"F1 Score: \", f1score)"
      ],
      "metadata": {
        "id": "3NTApY7cYVCN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OLYVjZBq9az"
      },
      "source": [
        "#### Visualização de métricas da avaliação do modelo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fazendo as previsões no conjunto de teste usando o test_generator\n",
        "y_pred = model.predict(test_generator)\n",
        "\n",
        "# Convertendo as previsões de probabilidades para os rótulos\n",
        "y_pred_labels = np.argmax(y_pred, axis=1)\n",
        "y_pred_labels = [class_names[label] for label in y_pred_labels]\n",
        "\n",
        "# Obtendo os rótulos verdadeiros do conjunto de teste\n",
        "y_true_labels = df_test['animal'].tolist()\n",
        "\n",
        "# Calculando as métricas\n",
        "accuracy = skm.accuracy_score(y_true_labels, y_pred_labels)\n",
        "precision = skm.precision_score(y_true_labels, y_pred_labels, average='weighted')\n",
        "recall = skm.recall_score(y_true_labels, y_pred_labels, average='weighted')\n",
        "f1score = skm.f1_score(y_true_labels, y_pred_labels, average='weighted')\n",
        "\n",
        "print(\"Accuracy: \", accuracy)\n",
        "print(\"Precision: \", precision)\n",
        "print(\"Recall: \", recall)\n",
        "print(\"F1 Score: \", f1score)"
      ],
      "metadata": {
        "id": "D67pzU8436Rz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    Esta função imprime e plota a matriz de confusão.\n",
        "    A normalização pode ser aplicada definindo `normalize=True`.\n",
        "    \"\"\"\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Matriz de confusão normalizada\")\n",
        "    else:\n",
        "        print('Matriz de confusão sem normalização')\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('Label real')\n",
        "    plt.xlabel('Label predito')"
      ],
      "metadata": {
        "id": "bqSSpwli5kc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnf_matrix = confusion_matrix(y_true_labels, y_pred_labels, labels=classes)\n",
        "np.set_printoptions(precision=2)\n",
        "\n",
        "plt.figure()\n",
        "plot_confusion_matrix(cnf_matrix,\n",
        "                      classes=['vaca', 'cavalo', 'galinha', 'elefante'],\n",
        "                      normalize= False,\n",
        "                      title='Matriz real x predição')"
      ],
      "metadata": {
        "id": "0sjHMkPb5L6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vHDPnAuQa8gr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "EaB8zIk_q9a0"
      },
      "outputs": [],
      "source": [
        "# Calculando as métricas\n",
        "accuracy = skm.accuracy_score(y_true, y_pred)\n",
        "precision = skm.precision_score(y_true, y_pred, average='weighted')\n",
        "recall = skm.recall_score(y_true, y_pred, average='weighted')\n",
        "f1score = skm.f1_score(y_true, y_pred, average='weighted')\n",
        "\n",
        "print(\"Accuracy: \", accuracy)\n",
        "print(\"Precision: \", precision)\n",
        "print(\"Recall: \", recall)\n",
        "print(\"F1 Score: \", f1score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2YwkeXNq9a0"
      },
      "source": [
        "Matriz de confusão para identificar onde o modelo de deep learning acertou e errou na classificação das imagens de teste"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "Kn77MZ5qq9a1"
      },
      "outputs": [],
      "source": [
        "cnf_matrix = confusion_matrix(y_true, y_pred, labels=class_names)\n",
        "np.set_printoptions(precision=2)\n",
        "\n",
        "plt.figure()\n",
        "plot_confusion_matrix(cnf_matrix,\n",
        "                      classes=['vaca', 'cavalo', 'galinha', 'elefaante'],\n",
        "                      normalize= False,\n",
        "                      title='Matriz real x predição')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOWCttsMq9a1"
      },
      "source": [
        "## 7. Exportação do modelo de deep learning para posterior uso\n",
        "\n",
        "Salvando o modelo de deep learning que foi treinado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g3w-67B0q9a1"
      },
      "outputs": [],
      "source": [
        "# obtendo a data e hora atual\n",
        "now = datetime.now()\n",
        "\n",
        "# Definição do formato\n",
        "format = '%Y-%m-%dT%H%M'\n",
        "\n",
        "# Converter a data e hora em uma string com o formato especificado\n",
        "formatted_datetime = now.strftime(format)\n",
        "\n",
        "path_model = 'datasets/house/trained_models'\n",
        "\n",
        "name_model = 'trained_model_' + formatted_datetime + '.h5'\n",
        "\n",
        "# salvando o modelo\n",
        "model.save(\"%s/%s\" % (path_model, name_model))\n",
        "print(\"Modelo salvo com o nome: \", name_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjujNrKUq9a1"
      },
      "source": [
        "## 8. Teste do modelo exportado\n",
        "\n",
        "Carregando o modelo salvo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "leVjvIH6q9a2"
      },
      "outputs": [],
      "source": [
        "loaded_model = keras.models.load_model(\"%s/%s\" % (path_model, name_model))\n",
        "print(\"Modelo %s carregado com sucesso\" % (name_model))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugqghl6eq9a2"
      },
      "source": [
        "Executando o modelo exportado para acompanhar as classificações de cada uma das imagens de teste"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86DpWFNLq9a2"
      },
      "outputs": [],
      "source": [
        "parent_dir = 'datasets/house/test'\n",
        "count_images = 0\n",
        "class_names = ['bath',\n",
        "               'bed',\n",
        "               'din',\n",
        "               'kitchen',\n",
        "               'living']\n",
        "y_pred = list()\n",
        "y_true = list()\n",
        "\n",
        "# Percorrendo a pasta onde estão salvas as imagens de teste\n",
        "for subdir, dirs, files in os.walk(parent_dir):\n",
        "\n",
        "    for file in files:\n",
        "\n",
        "        if file.endswith('.png') or file.endswith('.jpg'):\n",
        "\n",
        "            count_images+=1\n",
        "            split_path = os.path.join(subdir, file).split('/')\n",
        "            label = split_path[3]\n",
        "            y_true.append(label)\n",
        "\n",
        "            img_path = os.path.join(subdir, file)\n",
        "            display(Image(filename=img_path, width=300))\n",
        "\n",
        "            img = image.load_img(img_path, target_size=(img_height,img_width))\n",
        "            x = image.img_to_array(img)\n",
        "            x = np.expand_dims(x, axis=0)\n",
        "            x = x.astype('float32') / 255.0\n",
        "\n",
        "            # Previsão\n",
        "            prediction = loaded_model.predict(x)\n",
        "\n",
        "            # Printando as saídas do modelo\n",
        "            predicted_class = np.argmax(prediction[0])\n",
        "            probability = prediction[0][predicted_class]\n",
        "            y_pred.append(class_names[predicted_class])\n",
        "            print(\"Label:\", label)\n",
        "            print(\"Previsão:\", class_names[predicted_class])\n",
        "            print(\"Probabilidade:\", probability)\n",
        "            print(\"\\n\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "cell_execution_strategy": "setup",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}