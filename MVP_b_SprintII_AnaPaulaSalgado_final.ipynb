{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ana-PPS/data-and-analytcs/blob/MVP_II/MVP_b_SprintII_AnaPaulaSalgado_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRILPeMPQymh"
      },
      "source": [
        "# **PUC Rio: Pós Graduação em Ciência de Dados e Analytics**\n",
        "\n",
        "**MVP Sprint II: Machine Learning & Analytics**\n",
        "**Parte b**\n",
        "\n",
        "Aluna: Ana Paula Pinheiro Salgado\n",
        "\n",
        "Julho/2023"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVbw0VVzq9al"
      },
      "source": [
        "## Seção I: Introdução\n",
        "**Classificador de imagens multiclasse - Animais**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJrdI-umq9an"
      },
      "source": [
        "**Contexto:** Temos um conjunto de imagens que representam 4 categorias de animais: vaca, galinha, cavalo e elefante. O objetivo deste notebook é criar um modelo de visão computacional que consiga classificar uma imagem dentre essas categorias.\n",
        "\n",
        "**Estrutura:** O notebook encontra-se dividido da seguinte forma:\n",
        "\n",
        "- Importação das bibliotecas\n",
        "- Acesso e tratamento dos dados que serão a entrada do modelo de deep learning\n",
        "- Configuração do modelo de deep learning usando uma rede neural convolucional simples com Keras\n",
        "- Treinamento do modelo de deep learning\n",
        "- Execução do modelo de deep learning treinado\n",
        "- Avaliação do modelo de deep learning\n",
        "- Exportação do modelo de deep learning\n",
        "- Teste do modelo exportado\n",
        "\n",
        "**Dataset:** A partir do dataset original, baixado do Kaggle (https://www.kaggle.com/datasets/alessiocorrado99/animals10), foram selecionadas apenas 4 pastas de imagens de forma a otimizar o tempo de execução do MVP."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyu90mQIq9ap"
      },
      "source": [
        "### Importando as bibliotecas necessárias para executar o notebook\n",
        "(Serão utilizadas as bibliotecas pandas e numpy, para a manipulação dos dados; matplotlib, para geração de gráficos; os, para manipulação de pastas e diretórios e bibliotecas voltadas para Machine e Deep Learning, tais como Keras, Tensor Flow e Scikit-Learn)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xuBOf55bq9ap"
      },
      "outputs": [],
      "source": [
        "# para usar o Google Drive\n",
        "!pip install -q gdown\n",
        "import gdown\n",
        "from google.colab import drive\n",
        "\n",
        "#para acessar e manipular arquivos, diretórios e estrutra de dados\n",
        "import os\n",
        "import shutil\n",
        "from zipfile import ZipFile\n",
        "import pandas as pd\n",
        "\n",
        "# cálculos numéricos e operações matemáticas e trabalhar com números aleatórios\n",
        "import numpy as np\n",
        "import math\n",
        "import random\n",
        "\n",
        "# bibliotecas do keras para pré-processamento, modelos convolucionais e otimizadores dos modelos\n",
        "!pip install -q tensorflow\n",
        "!pip install -q keras\n",
        "from tensorflow import keras\n",
        "from keras.optimizers import Adam\n",
        "from keras.preprocessing import image\n",
        "#from keras.layers.experimental import preprocessing\n",
        "from keras.layers import BatchNormalization\n",
        "from keras import layers,models,Model\n",
        "#from keras.applications import VGG16\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras import layers\n",
        "from keras import models\n",
        "from keras.preprocessing import image as keras_image\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "import tensorflow as tf\n",
        "\n",
        "# sckit-learn para pré-processamento e uso de métricas em machine learning\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.utils import class_weight, shuffle\n",
        "from sklearn.metrics import confusion_matrix, classification_report, balanced_accuracy_score\n",
        "import sklearn.metrics as skm\n",
        "\n",
        "# plotagem de gráficos, visualizações e imagens\n",
        "from PIL import Image\n",
        "import matplotlib.image as mpimg\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from IPython.display import display\n",
        "from skimage.io import imread\n",
        "from skimage.transform import resize\n",
        "\n",
        "# iterações\n",
        "import itertools\n",
        "\n",
        "# para uso de data e hora\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEkc_j58RU6i"
      },
      "source": [
        "##Seção II: Acessando e tratando os dados que serão a entrada do modelo de deep learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UcY4ZgosYDsS"
      },
      "outputs": [],
      "source": [
        "# Baixando a pasta compactada para o notebook\n",
        "file_id = \"1bg_JhbuWv2lCCDyrfGI2yr5dROxbijYQ\"\n",
        "\n",
        "folder_path = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "output = \"4animais.zip\"\n",
        "gdown.download(folder_path, output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7X1Ott8sn5vS"
      },
      "outputs": [],
      "source": [
        "# Descompactando o arquivo\n",
        "with ZipFile('4animais.zip', 'r') as zip_object:\n",
        "  zip_object.extractall()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sivZTePOYHGl"
      },
      "outputs": [],
      "source": [
        "# Reunindo todos os arquivos em uma pasta para transformar em um dataset\n",
        "\n",
        "path = \"/content/\"\n",
        "\n",
        "classes = ['cavalo', 'elefante', 'galinha', 'vaca']\n",
        "\n",
        "foldernames = os.listdir(path)\n",
        "\n",
        "data = {\"images\": [], \"animal\": []}\n",
        "\n",
        "for folder in classes:\n",
        "    folderpath = os.path.join(path, folder)\n",
        "    filelist = os.listdir(folderpath)\n",
        "    for file in filelist:\n",
        "        fpath = os.path.join(folderpath, file)\n",
        "        data[\"images\"].append(fpath)\n",
        "        data[\"animal\"].append(folder)\n",
        "\n",
        "\n",
        "df = pd.DataFrame(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ZLR0vKW9Nw0"
      },
      "outputs": [],
      "source": [
        "print(df.head)\n",
        "print(df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gP-JbVIqYHBu"
      },
      "outputs": [],
      "source": [
        "#Resumo do dataset criado\n",
        "\n",
        "print(\"Tamanho do dataset: \", df.shape)\n",
        "print(\"_______________________________________\")\n",
        "print(\"Valores null: \")\n",
        "print(df.isnull().sum())\n",
        "print(\"_______________________________________\")\n",
        "print(\"Valores únicos: \")\n",
        "print(df.nunique())\n",
        "\n",
        "print(\"_______________________________________\")\n",
        "print(\"Qnt de imagens por categoria : \")\n",
        "print(df.animal.value_counts())\n",
        "\n",
        "print(\"_______________________________________\")\n",
        "print(\"Informação do dataset: \")\n",
        "print(df.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G07ArvIwYG5f"
      },
      "outputs": [],
      "source": [
        "#Visualizando algumas imagens do conjunto de dados\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(3*4, 3*4)\n",
        "for i, row in df.sample(n=10).reset_index().iterrows():\n",
        "    plt.subplot(2,5,i+1)\n",
        "    image_path = row['images']\n",
        "    image = Image.open(image_path)\n",
        "    plt.imshow(image)\n",
        "    plt.title(row[\"animal\"])\n",
        "    plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yNm6NziRZYDK"
      },
      "outputs": [],
      "source": [
        "# Separação do dataframe em treino, teste e validação\n",
        "df_train, Temp_df = train_test_split(df, train_size=0.7, random_state=13, shuffle=True)\n",
        "df_test, df_val = train_test_split(Temp_df, test_size=0.6, random_state=30, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UMrAekGIZq9b"
      },
      "outputs": [],
      "source": [
        "#Resumo da separação do conjunto de dados\n",
        "print(\"#########Train##############\")\n",
        "print(df_train.head())\n",
        "print(df_train.shape)\n",
        "num_imagens_por_classe = df_train['animal'].value_counts()\n",
        "print(num_imagens_por_classe)\n",
        "print(\"#########Test###############\")\n",
        "print(df_test.head())\n",
        "print(df_test.shape)\n",
        "num_imagens_por_classe = df_test['animal'].value_counts()\n",
        "print(num_imagens_por_classe)\n",
        "print(\"#########Validação###############\")\n",
        "print(df_val.head())\n",
        "print(df_val.shape)\n",
        "num_imagens_por_classe = df_val['animal'].value_counts()\n",
        "print(num_imagens_por_classe)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rs2lLQhJq9au"
      },
      "source": [
        "###  Preparação dos dados\n",
        "\n",
        "No presente trabalho, será usado o método flow_from_dataframe() para gerar imagens aumentadas a partir de um dataframe, que aponta para as imagens originais. O método recebe parâmetros como o dataframe, o diretório com as imagens, o tamanho do lote (batch size) e o modo de classe (categorical, em função da quantidade de classes), entre outros.\n",
        "\n",
        "O Data Augmentation aplica transformações aleatórias nas imagens existentes, o que ajuda a evitar overfitting e torna o modelo mais robusto, expondo-o a uma variedade maior de variações nas imagens.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n5IcBa8Qq9at"
      },
      "outputs": [],
      "source": [
        "# Definindo o tamanho do batch, a dimensão das imagens e a quantidade de épocas\n",
        "batch_size = 20\n",
        "img_height = 224\n",
        "img_width = 224\n",
        "num_classes = 4\n",
        "epochs = 15  # Para o MVP, utilizou-se o número pequeno para reduzir o tempo computacional"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rlIamgUyaLAQ"
      },
      "outputs": [],
      "source": [
        "train_datagen=ImageDataGenerator(rescale=1./255,\n",
        "                             rotation_range=40,\n",
        "                             shear_range=0.2,\n",
        "                             zoom_range=0.2,\n",
        "                             horizontal_flip=True,\n",
        "                             fill_mode='nearest')\n",
        "\n",
        "val_datagen=ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "test_datagen=ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "\n",
        "train_generator=train_datagen.flow_from_dataframe(\n",
        "    dataframe=df_train,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    x_col='images',\n",
        "    y_col='animal',\n",
        "    color_mode ='rgb',\n",
        "    seed = 13,\n",
        "    shuffle=False\n",
        "    )\n",
        "\n",
        "val_generator=val_datagen.flow_from_dataframe(\n",
        "    dataframe=df_val,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    x_col='images',\n",
        "    y_col='animal',\n",
        "    color_mode ='rgb',\n",
        "    shuffle=False\n",
        "    )\n",
        "\n",
        "test_generator=test_datagen.flow_from_dataframe(\n",
        "    dataframe = df_test,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    x_col='images',\n",
        "    y_col='animal',\n",
        "    color_mode ='rgb',\n",
        "    shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zs7GKCouSY2E"
      },
      "source": [
        "## Seção III: Configuração do Modelo de Deep Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiKBVra3q9aw"
      },
      "source": [
        "### Configuração de um modelo de deep learning usando uma rede neural convolucional (CNN) simples com a biblioteca Keras\n",
        "\n",
        "O modelo será construído com 4 camadas convolucionais 2D, com 32, 64, 128 e 256 filtros e com função de ativação `ReLU`. Inseridas também 2 camadas de Dropout para tentar melhorar a generalização e reduzir overfitting. Na sequência é adicionada uma camada `softmax` com a mesma função de ativação."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZ6gQ8kOL2R9"
      },
      "outputs": [],
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(img_height,img_width,3)),\n",
        "    keras.layers.Conv2D(32, (3, 3), padding='same', activation='relu'),\n",
        "    keras.layers.MaxPooling2D((2, 2)),\n",
        "    keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu'),\n",
        "    keras.layers.MaxPooling2D((2, 2)),\n",
        "    keras.layers.Conv2D(128, (3, 3), padding='same', activation='relu'),\n",
        "    keras.layers.MaxPooling2D((2, 2)),\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(256, activation='relu'),  # Aumentar a complexidade com mais unidades\n",
        "    keras.layers.Dropout(0.2),  # Incluir mais uma camada de dropout\n",
        "    keras.layers.Dense(128, activation='relu'),  # Adicionar outra camada Dense\n",
        "    keras.layers.Dense(num_classes, activation='softmax')\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YPitAuTOsNsc"
      },
      "outputs": [],
      "source": [
        "# Resumindo o modelo que será utilizado\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiQl2rsPsUI6"
      },
      "source": [
        "### Treinamento com o modelo otimizado\n",
        "\n",
        "Utilizado o modelo Adam para otimização, perda do tipo de entropia cruzada, por sem um problema de classificação multiclasse e métrica de acurácia para avaliação do modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qAfZa9AyqS3k"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_MN30gGqT7a"
      },
      "outputs": [],
      "source": [
        "history = model.fit(train_generator,\n",
        "                    steps_per_epoch=len(train_generator),\n",
        "                    epochs=epochs,\n",
        "                    validation_data=val_generator,\n",
        "                    validation_steps=len(val_generator),\n",
        "                    callbacks=[EarlyStopping(monitor='val_loss', patience=5,\n",
        "                                             restore_best_weights=True)])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJ2PWoW2ACFX"
      },
      "source": [
        "#### Visualização de métricas da avaliação do modelo treinado\n",
        "\n",
        "A entropia cruzada mede a diferença entre duas distribuições de probabilidade (verdadeira e falsa) sobre um mesmo conjunto de dados.\n",
        "\n",
        "A acurácia é a proximidade de um resultado predito com o seu valor de referência real.\n",
        "\n",
        "Conforme se aumenta o número de épocas, espera-se que o parâmetro loss diminua (para o dataset de treino e também para o de validação) e que a acurácia aumente.\n",
        "\n",
        "Além disso, é possível inferir sobre under ou overfitting do modelo a depender dos resultados para os conjuntos de treino e validação."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NKTezeW-_xex"
      },
      "outputs": [],
      "source": [
        "accuracy = history.history['accuracy']\n",
        "val_accuracy = history.history['val_accuracy']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_range = range(epochs)\n",
        "\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, accuracy, label = 'Training Accuracy')\n",
        "plt.plot(epochs_range, val_accuracy, label = 'Validation Accuracy')\n",
        "plt.title('Training vs Validation Accuracy')\n",
        "plt.xlabel('Epochs Number')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, loss, label = 'Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label = 'Validation Loss')\n",
        "plt.title('Training vs Validation Loss')\n",
        "plt.xlabel('Epochs Number')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhbNC_28q9ay"
      },
      "source": [
        "## Seção IV: Execução do modelo de deep learning treinado nas imagens de teste"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XH06dYQgdPy4"
      },
      "source": [
        "#### Execução do modelo com imagens do dataframe de teste\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Rfkcxw8cWZJ"
      },
      "outputs": [],
      "source": [
        "count_images = 0\n",
        "class_names = ['cavalo',\n",
        "               'elefante',\n",
        "               'galinha',\n",
        "               'vaca']\n",
        "y_pred = list() # para armazenar as categorias preditas das imagens do dataset de teste\n",
        "y_true = list() # para armazenar as categorias reais das imagens do dataset de teste"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ia-G6AqKvex"
      },
      "outputs": [],
      "source": [
        "# Reimportando a biblioteca treino, pois, durante os testes com o notebook, esta célula apresentava erro\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "\n",
        "# Percorrendo as imagens do dataframe de teste para aplicação de modelo treinado\n",
        "for img_path, label in zip(df_test['images'], df_test['animal']):\n",
        "     _, file_extension = os.path.splitext(img_path)\n",
        "     if file_extension.lower() in ['.jpeg', '.jpg', '.png']:\n",
        "         count_images += 1\n",
        "\n",
        "         split_path = img_path.split('/')\n",
        "         label = split_path[2]\n",
        "         y_true.append(label)\n",
        "\n",
        "         img = Image.open(img_path).resize((300, 300))\n",
        "         display(img)\n",
        "\n",
        "         img = load_img(img_path, target_size=(img_height, img_width))\n",
        "         x = img_to_array(img)\n",
        "         x = np.expand_dims(x, axis=0)\n",
        "         x = x.astype('float32') / 255.0\n",
        "\n",
        "         # Predições\n",
        "         prediction = model.predict(x)\n",
        "\n",
        "         # Printando as saídas do modelo\n",
        "         predicted_class = np.argmax(prediction[0])\n",
        "         probability = prediction[0][predicted_class]\n",
        "         y_pred.append(class_names[predicted_class])\n",
        "         print(\"Categoria Real: \", label)\n",
        "         print(\"Categoria Predita: \", class_names[predicted_class])\n",
        "         print(\"Probabilidade: \", probability)\n",
        "         print(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3NTApY7cYVCN"
      },
      "outputs": [],
      "source": [
        "# Calculando as métricas do modelo aplicado as imagens de teste\n",
        "accuracy = skm.accuracy_score(y_true, y_pred)\n",
        "precision = skm.precision_score(y_true, y_pred, average='weighted')\n",
        "recall = skm.recall_score(y_true, y_pred, average='weighted')\n",
        "f1score = skm.f1_score(y_true, y_pred, average='weighted')\n",
        "\n",
        "# Arredondar os valores para duas casas decimais\n",
        "accuracy = round(accuracy, 2)\n",
        "precision = round(precision, 2)\n",
        "recall = round(recall, 2)\n",
        "f1score = round(f1score, 2)\n",
        "\n",
        "print(\"Accuracy: \", accuracy)\n",
        "print(\"Precision: \", precision)\n",
        "print(\"Recall: \", recall)\n",
        "print(\"F1 Score: \", f1score)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classification_rep = classification_report(y_true, y_pred, target_names=class_names)\n",
        "print(\"\\nRelatório de Classificação:\")\n",
        "print(classification_rep)"
      ],
      "metadata": {
        "id": "3K0CLOixmReN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O modelo apresentou uma acurácia de 79% no geral, o que significa que a maioria dos exemplos do conjunto de dados foram classificados corretamente.\n",
        "A classe 'vaca' apresenta um índice de recall bem inferior aos demais, o que denota dificuldade do modelo em identificar esta classe."
      ],
      "metadata": {
        "id": "ABaqc5VzFUZB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Matriz de Confusão oferece um detalhamento do desempenho do modelo, mostrando, para cada classe, o número de classificações corretas em relação ao número de classificações indicadas pelo modelo"
      ],
      "metadata": {
        "id": "SkcRqNKJ_aK0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = np.array(y_pred)\n",
        "y_true = np.array(y_true)\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# Definindo função para plotar a matriz de confusão\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    Esta função imprime e plota a matriz de confusão.\n",
        "    A normalização pode ser aplicada definindo `normalize=True`.\n",
        "    \"\"\"\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Matriz de confusão normalizada\")\n",
        "    else:\n",
        "        print('Matriz de confusão sem normalização')\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt_str = '{:d}' if not normalize else '{:.2f}'\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, fmt_str.format(cm[i, j]),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('Label real')\n",
        "    plt.xlabel('Label predito')\n",
        "\n",
        "    plt.figure(figsize=(12,12))\n",
        "plot_confusion_matrix(cm, classes, normalize=False, title='Matriz de Confusão')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "otq2qge2maGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMo6Kf1kbdMk"
      },
      "source": [
        "#### Execução do modelo treinado com test generator (Essa parte foi utilizada pois o código de teste da sessão anterior não estava funcionando adequadamente, ficando apenas de histórico neste notebook)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R2QPCOHuMNIe"
      },
      "outputs": [],
      "source": [
        "# y_true_tg = test_generator.classes\n",
        "# y_pred_tg = np.argmax(model.predict(test_generator), axis=1)\n",
        "\n",
        "# classes = dict(zip(test_generator.class_indices.values(), test_generator.class_indices.keys()))\n",
        "# Predictions = pd.DataFrame({\"Image Index\": list(range(len(test_generator.labels))), # Índice da imagem no conjunto de teste\n",
        "#                             \"Test Labels\": test_generator.labels, # Rótulos reais das imagens no conjunto de teste\n",
        "#                             \"Test Classes\": [classes[i] for i in test_generator.labels], # Classes correspondentes aos rótulos reais\n",
        "#                             \"Prediction Labels\": y_pred_tg, # Rótulos previstos pelo modelo\n",
        "#                             \"Prediction Classes\": [classes[i] for i in y_pred_tg], # Classes correspondentes aos rótulos previstos\n",
        "#                             \"Path\": test_generator.filepaths, #Caminho dos arquivos de imagem\n",
        "#                             \"Prediction Probability\": [x for x in np.asarray(np.max(model.predict(test_generator), axis=1))] #Probabilidade da classe prevista pelo modelo\n",
        "#                            })\n",
        "# Predictions.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9YJ7P5-Nw1Y"
      },
      "source": [
        "Plotando um subconjunto das imagens de teste com as respectivas previsões"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBX8E6D9NwTy"
      },
      "outputs": [],
      "source": [
        "# Obtendo uma amostra aleatória de 10 linhas do DataFrame Predict\n",
        "\n",
        "# sample_predictions = Predictions.sample(n=10)\n",
        "\n",
        "# for index, row in sample_predictions.iterrows():\n",
        "#     # Obtendo as informações necessárias para visualização e impressão\n",
        "#     image = plt.imread(row['Path'])\n",
        "#     true_label = row['Test Classes']\n",
        "#     predicted_label = row['Prediction Classes']\n",
        "#     probability = row['Prediction Probability']\n",
        "\n",
        "#     # Arredondando a probabilidade para duas casas decimais\n",
        "#     probability = round(probability, 2)\n",
        "\n",
        "#     # Configuração da figura para exibir a imagem\n",
        "#     plt.figure()\n",
        "#     plt.imshow(image)\n",
        "#     plt.axis('off')\n",
        "#     plt.show()\n",
        "\n",
        "#     # Impressão dos resultados da classificação\n",
        "#     print(\"True Label: \", true_label)\n",
        "#     print(\"Predicted Label: \", predicted_label)\n",
        "#     print(\"Probability: \", probability)\n",
        "#     print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jgl-u96fk0QX"
      },
      "outputs": [],
      "source": [
        "# results = model.evaluate(test_generator, verbose=0)\n",
        "\n",
        "# print(\"    Test Loss: {:.2f}\".format(results[0]))\n",
        "# print(\"Test Accuracy: {:.2f}%\".format(results[1] * 100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OURA8dbsa9k2"
      },
      "outputs": [],
      "source": [
        "# Calculando as métricas do modelo aplicado as imagens do teste generator\n",
        "# accuracy = skm.accuracy_score(y_true_tg, y_pred_tg)\n",
        "# precision = skm.precision_score(y_true_tg, y_pred_tg, average='weighted')\n",
        "# recall = skm.recall_score(y_true_tg, y_pred_tg, average='weighted')\n",
        "# f1score = skm.f1_score(y_true_tg, y_pred_tg, average='weighted')\n",
        "\n",
        "# # Arredondar os valores para duas casas decimais\n",
        "# accuracy = round(accuracy, 2)\n",
        "# precision = round(precision, 2)\n",
        "# recall = round(recall, 2)\n",
        "# f1score = round(f1score, 2)\n",
        "\n",
        "# print(\"Accuracy: \", accuracy)\n",
        "# print(\"Precision: \", precision)\n",
        "# print(\"Recall: \", recall)\n",
        "# print(\"F1 Score: \", f1score)\n",
        "# print(classification_report(y_true_tg, y_pred_tg, target_names=test_generator.class_indices.keys()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqSSpwli5kc6"
      },
      "outputs": [],
      "source": [
        "# Definindo função para plotar a matriz de confusão\n",
        "# def plot_confusion_matrix(cm, classes,\n",
        "#                           normalize=False,\n",
        "#                           title='Confusion matrix',\n",
        "#                           cmap=plt.cm.Blues):\n",
        "#     \"\"\"\n",
        "#     Esta função imprime e plota a matriz de confusão.\n",
        "#     A normalização pode ser aplicada definindo `normalize=True`.\n",
        "#     \"\"\"\n",
        "#     if normalize:\n",
        "#         cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "#         print(\"Matriz de confusão normalizada\")\n",
        "#     else:\n",
        "#         print('Matriz de confusão sem normalização')\n",
        "\n",
        "#     plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "#     plt.title(title)\n",
        "#     plt.colorbar()\n",
        "#     tick_marks = np.arange(len(classes))\n",
        "#     plt.xticks(tick_marks, classes, rotation=45)\n",
        "#     plt.yticks(tick_marks, classes)\n",
        "\n",
        "#     fmt_str = '{:d}' if not normalize else '{:.2f}'\n",
        "\n",
        "#     thresh = cm.max() / 2.\n",
        "#     for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "#         plt.text(j, i, fmt_str.format(cm[i, j]),\n",
        "#                  horizontalalignment=\"center\",\n",
        "#                  color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "#     plt.tight_layout()\n",
        "#     plt.ylabel('Label real')\n",
        "#     plt.xlabel('Label predito')\n",
        "\n",
        "# class_names = list(test_generator.class_indices.values())\n",
        "# y_pred_tg = Predictions['Prediction Labels']\n",
        "# y_test_tg = Predictions['Test Labels']\n",
        "# Map_class = test_generator.class_indices\n",
        "\n",
        "# cnf_matrix = confusion_matrix(y_true_tg, y_pred_tg, labels=class_names)\n",
        "# np.set_printoptions(precision=2)\n",
        "\n",
        "# plt.figure()\n",
        "# plot_confusion_matrix(cnf_matrix,\n",
        "#                       classes=Map_class,\n",
        "#                       normalize=False,\n",
        "#                       title='Matriz real x predição')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOWCttsMq9a1"
      },
      "source": [
        "## Seção V: Exportação do modelo de deep learning para posterior uso\n",
        "\n",
        "Salvando o modelo de deep learning que foi treinado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g3w-67B0q9a1"
      },
      "outputs": [],
      "source": [
        "# obtendo a data e hora atual\n",
        "now = datetime.now()\n",
        "\n",
        "# Definição do formato\n",
        "format = '%Y-%m-%dT%H%M'\n",
        "\n",
        "# Converter a data e hora em uma string com o formato especificado\n",
        "formatted_datetime = now.strftime(format)\n",
        "\n",
        "path_model = 'content/trained_models'\n",
        "\n",
        "name_model = 'trained_model_' + formatted_datetime + '.h5'\n",
        "\n",
        "# salvando o modelo\n",
        "model.save(\"%s/%s\" % (path_model, name_model))\n",
        "print(\"Modelo salvo com o nome: \", name_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjujNrKUq9a1"
      },
      "source": [
        "## Seção VI: Teste do modelo exportado\n",
        "\n",
        "Carregando o modelo salvo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "leVjvIH6q9a2"
      },
      "outputs": [],
      "source": [
        "loaded_model = keras.models.load_model(\"%s/%s\" % (path_model, name_model))\n",
        "print(\"Modelo %s carregado com sucesso\" % (name_model))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugqghl6eq9a2"
      },
      "source": [
        "Executando o modelo exportado para acompanhar as classificações de cada uma das imagens de teste"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86DpWFNLq9a2"
      },
      "outputs": [],
      "source": [
        "# Importando a pasta com as novas fotos\n",
        "from google.colab import drive\n",
        "\n",
        "# Montar o Google Drive\n",
        "drive.mount('content/drive')\n",
        "\n",
        "parent_dir = 'content/drive/MyDrive/dataset/usp=drive_link'\n",
        "count = 0\n",
        "class_names = ['cavalo', 'elefante', 'galinha', 'vaca']\n",
        "\n",
        "y_pred = list()\n",
        "y_true - list()\n",
        "\n",
        "for subdir, dirs, files in os.walk(parent_dir):\n",
        "\n",
        "    for file in files:\n",
        "\n",
        "        if file.endswith('.png') or file.endswith('.jpg'):\n",
        "\n",
        "            count_images+=1\n",
        "            split_path = os.path.join(subdir, file).split('/')\n",
        "            label = split_path[-2]\n",
        "            y_true.append(label)\n",
        "\n",
        "            img_path = os.path.join(subdir, file)\n",
        "            display(ipimg(filename=img_path, width=300))\n",
        "\n",
        "            img = image.load_img(img_path, target_size=(img_height, img_width))\n",
        "            x = image.img_to_array(img)\n",
        "            x = np.expand_dims(x, axis=0)\n",
        "            x = x.astype('float32') / 255.0\n",
        "\n",
        "            # Previsão\n",
        "            prediction = loaded_model.predict(x)\n",
        "\n",
        "            # Printando as saídas do modelo\n",
        "            predicted_class = np.argmax(prediction[0])\n",
        "            probability = prediction[0][predicted_class]\n",
        "            y_pred.append(class_names[predicted_class])\n",
        "            print(\"Label:\", label)\n",
        "            print(\"Previsão:\", class_names[predicted_class])\n",
        "            print(\"Probabilidade:\", probability)\n",
        "            print(\"\\n\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "rVbw0VVzq9al",
        "xyu90mQIq9ap",
        "SMo6Kf1kbdMk"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}